<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>cf7b6c28c5914c068f3196b2df20ab15</title>
  <style>
    html {
      line-height: 1.5;
      font-family: Georgia, serif;
      font-size: 20px;
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      word-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 1em;
      }
    }
    @media print {
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, 'Lucida Console', Consolas, monospace;
      font-size: 85%;
      margin: 0;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      {   }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span.al { color: #ff0000; font-weight: bold; } /* Alert */
    code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
    code span.at { color: #7d9029; } /* Attribute */
    code span.bn { color: #40a070; } /* BaseN */
    code span.bu { } /* BuiltIn */
    code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #4070a0; } /* Char */
    code span.cn { color: #880000; } /* Constant */
    code span.co { color: #60a0b0; font-style: italic; } /* Comment */
    code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
    code span.do { color: #ba2121; font-style: italic; } /* Documentation */
    code span.dt { color: #902000; } /* DataType */
    code span.dv { color: #40a070; } /* DecVal */
    code span.er { color: #ff0000; font-weight: bold; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #40a070; } /* Float */
    code span.fu { color: #06287e; } /* Function */
    code span.im { } /* Import */
    code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
    code span.kw { color: #007020; font-weight: bold; } /* Keyword */
    code span.op { color: #666666; } /* Operator */
    code span.ot { color: #007020; } /* Other */
    code span.pp { color: #bc7a00; } /* Preprocessor */
    code span.sc { color: #4070a0; } /* SpecialChar */
    code span.ss { color: #bb6688; } /* SpecialString */
    code span.st { color: #4070a0; } /* String */
    code span.va { color: #19177c; } /* Variable */
    code span.vs { color: #4070a0; } /* VerbatimString */
    code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
    .display.math{display: block; text-align: center; margin: 0.5rem auto;}
  </style>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<section id="machine-learning---boosting" class="cell markdown" id="LTs7BG93CbmP">
<h1>Machine Learning - Boosting</h1>
<h2 id="masaccio-braun">Masaccio Braun</h2>
<p>In the past, we have used machine learning methods that implement a single <em>learner</em> i.e. linear regressor, decision tree regressor, etc., as well as learning methods that implement multiple learners i.e. random forest regressor. Random forest models are known as 'ensemble' methods because they use a group of learners to enhance the performance of each individual learner and create a stronger, aggregate model. However, many of the individual learners in the ensemble tend to be <em>weak learners</em>, such that some tend to make poor predictions based on the observations they are specifically trained on. One way to combat this is through <strong>boosting</strong>. Boosting is a derivative of bagging, in which weak learners are trained simultaneously through a random selection, each having equal <em>weight</em> in predictive capability. Boosting differs in that the models have differing weights, since they are trained sequentially to compensate for weak learners by taking the efficacy of the previous model and increasing the weights of the data that the previous model had the highest error with.</p>
<p><img src="vertopal_de7364266f74463496a9b8fad12d9995/51c38e45f67558ed3101666399d0813b72394fd7.png" alt="boostingvsbagging" /></p>
<p>Source: <a href="https://towardsdatascience.com/what-is-boosting-in-machine-learning-2244aa196682" class="uri">https://towardsdatascience.com/what-is-boosting-in-machine-learning-2244aa196682</a></p>
<p>There are a multitude of methods that can be implemented to boost a model. In this exploration, I will test a random forest booster and a decision tree booster for locally weighted regression and a version of extreme gradient boosting regression, and compare the results of their predictions in two multivariate analyses.</p>
</section>
<div class="cell code" data-execution_count="1" data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}" id="cT6eOh_GNtFk" data-outputId="8da2660f-05ba-4b59-8fe3-29ac88707437">
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Update statsmodels</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="op">!</span> pip install statsmodels<span class="op">==</span><span class="fl">0.13.2</span></span></code></pre></div>
<div class="output stream stdout">
<pre><code>Collecting statsmodels==0.13.2
  Downloading statsmodels-0.13.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.8 MB)
ent already satisfied: numpy&gt;=1.17 in /usr/local/lib/python3.7/dist-packages (from statsmodels==0.13.2) (1.21.5)
Requirement already satisfied: scipy&gt;=1.3 in /usr/local/lib/python3.7/dist-packages (from statsmodels==0.13.2) (1.4.1)
Requirement already satisfied: packaging&gt;=21.3 in /usr/local/lib/python3.7/dist-packages (from statsmodels==0.13.2) (21.3)
Requirement already satisfied: pandas&gt;=0.25 in /usr/local/lib/python3.7/dist-packages (from statsmodels==0.13.2) (1.3.5)
Requirement already satisfied: patsy&gt;=0.5.2 in /usr/local/lib/python3.7/dist-packages (from statsmodels==0.13.2) (0.5.2)
Requirement already satisfied: pyparsing!=3.0.5,&gt;=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging&gt;=21.3-&gt;statsmodels==0.13.2) (3.0.7)
Requirement already satisfied: pytz&gt;=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas&gt;=0.25-&gt;statsmodels==0.13.2) (2018.9)
Requirement already satisfied: python-dateutil&gt;=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas&gt;=0.25-&gt;statsmodels==0.13.2) (2.8.2)
Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from patsy&gt;=0.5.2-&gt;statsmodels==0.13.2) (1.15.0)
Installing collected packages: statsmodels
  Attempting uninstall: statsmodels
    Found existing installation: statsmodels 0.10.2
    Uninstalling statsmodels-0.10.2:
      Successfully uninstalled statsmodels-0.10.2
Successfully installed statsmodels-0.13.2
</code></pre>
</div>
</div>
<div class="cell code" data-execution_count="2" id="lXnt67te04-C">
<div class="sourceCode" id="cb3"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Imports</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="co"># General libraries</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib <span class="im">as</span> mpl</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Linear algebra</span></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.linalg <span class="im">import</span> lstsq</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.sparse.linalg <span class="im">import</span> lsmr</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Interpolators</span></span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.interpolate <span class="im">import</span> interp1d, griddata, LinearNDInterpolator, NearestNDInterpolator</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Scalers</span></span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> StandardScaler</span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Cross Validation</span></span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> KFold</span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a><span class="co"># Model Evaluation</span></span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> mean_squared_error <span class="im">as</span> mse</span>
<span id="cb3-25"><a href="#cb3-25" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> mean_absolute_error <span class="im">as</span> mae</span>
<span id="cb3-26"><a href="#cb3-26" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> r2_score <span class="im">as</span> r2</span>
<span id="cb3-27"><a href="#cb3-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-28"><a href="#cb3-28" aria-hidden="true" tabindex="-1"></a><span class="co"># Regressors</span></span>
<span id="cb3-29"><a href="#cb3-29" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LinearRegression, Lasso, Ridge, ElasticNet</span>
<span id="cb3-30"><a href="#cb3-30" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.tree <span class="im">import</span> DecisionTreeRegressor</span>
<span id="cb3-31"><a href="#cb3-31" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.ensemble <span class="im">import</span> RandomForestRegressor</span>
<span id="cb3-32"><a href="#cb3-32" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> statsmodels.nonparametric.kernel_regression <span class="im">import</span> KernelReg</span>
<span id="cb3-33"><a href="#cb3-33" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> xgboost <span class="im">import</span> XGBRegressor</span>
<span id="cb3-34"><a href="#cb3-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-35"><a href="#cb3-35" aria-hidden="true" tabindex="-1"></a><span class="co"># Neural Network</span></span>
<span id="cb3-36"><a href="#cb3-36" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> keras.models <span class="im">import</span> Sequential</span>
<span id="cb3-37"><a href="#cb3-37" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> keras.layers <span class="im">import</span> Dense, Dropout</span>
<span id="cb3-38"><a href="#cb3-38" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> keras.callbacks <span class="im">import</span> EarlyStopping</span>
<span id="cb3-39"><a href="#cb3-39" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow.keras.optimizers <span class="im">import</span> Adam</span></code></pre></div>
</div>
<div class="cell code" data-execution_count="3" id="YKCq8GOt3tf4">
<div class="sourceCode" id="cb4"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># High-resolution images</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>matplotlib inline</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>config InlineBackend.figure_format <span class="op">=</span> <span class="st">&#39;retina&#39;</span></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>mpl.rcParams[<span class="st">&#39;figure.dpi&#39;</span>] <span class="op">=</span> <span class="dv">120</span></span></code></pre></div>
</div>
<div class="cell code" data-execution_count="4" data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}" id="UX7Al8yf3phd" data-outputId="35f1013f-2e54-4a99-a53c-7a1da784fbb3">
<div class="sourceCode" id="cb5"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Mount Google Drive</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> google.colab <span class="im">import</span> drive</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>drive.mount(<span class="st">&#39;/content/drive&#39;</span>)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Mounted at /content/drive
</code></pre>
</div>
</div>
<div class="cell code" data-execution_count="5" id="9JFXnqie3pk_">
<div class="sourceCode" id="cb7"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Tricubic Kernel</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> Tricubic(x):</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> <span class="bu">len</span>(x.shape) <span class="op">==</span> <span class="dv">1</span>:</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> x.reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>  d <span class="op">=</span> np.sqrt(np.<span class="bu">sum</span>(x<span class="op">**</span><span class="dv">2</span>,axis<span class="op">=</span><span class="dv">1</span>))</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> np.where(d<span class="op">&gt;</span><span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">70</span><span class="op">/</span><span class="dv">81</span><span class="op">*</span>(<span class="dv">1</span><span class="op">-</span>d<span class="op">**</span><span class="dv">3</span>)<span class="op">**</span><span class="dv">3</span>)</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Quartic Kernel</span></span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> Quartic(x):</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> <span class="bu">len</span>(x.shape) <span class="op">==</span> <span class="dv">1</span>:</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> x.reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)</span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>  d <span class="op">=</span> np.sqrt(np.<span class="bu">sum</span>(x<span class="op">**</span><span class="dv">2</span>,axis<span class="op">=</span><span class="dv">1</span>))</span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> np.where(d<span class="op">&gt;</span><span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">15</span><span class="op">/</span><span class="dv">16</span><span class="op">*</span>(<span class="dv">1</span><span class="op">-</span>d<span class="op">**</span><span class="dv">2</span>)<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Epanechnikov Kernel</span></span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> Epanechnikov(x):</span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> <span class="bu">len</span>(x.shape) <span class="op">==</span> <span class="dv">1</span>:</span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> x.reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)</span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a>  d <span class="op">=</span> np.sqrt(np.<span class="bu">sum</span>(x<span class="op">**</span><span class="dv">2</span>,axis<span class="op">=</span><span class="dv">1</span>))</span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> np.where(d<span class="op">&gt;</span><span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">3</span><span class="op">/</span><span class="dv">4</span><span class="op">*</span>(<span class="dv">1</span><span class="op">-</span>d<span class="op">**</span><span class="dv">2</span>)) </span></code></pre></div>
</div>
<section id="boston-housing-prices" class="cell markdown" id="BUf-LB0lCfUD">
<h3>Boston Housing Prices</h3>
<p>The first dataset I will try to model will be the Boston Housing Prices dataset, which can be found here: <a href="https://www.kaggle.com/prasadperera/the-boston-housing-dataset" class="uri">https://www.kaggle.com/prasadperera/the-boston-housing-dataset</a></p>
</section>
<div class="cell code" data-colab="{&quot;height&quot;:206,&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}" id="1O3cHZTd3poB" data-outputId="ef115222-7916-472b-82ec-b1c19c969a39">
<div class="sourceCode" id="cb8"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Load data</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>houses <span class="op">=</span> pd.read_csv(<span class="st">&#39;drive/MyDrive/Data/Boston Housing Prices.csv&#39;</span>)</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>houses.head()</span></code></pre></div>
<div class="output execute_result" data-execution_count="5">

  <div id="df-ef4c7c52-ab5d-49c8-b926-3ba678aca45d">
    <div class="colab-df-container">
      <div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>town</th>
      <th>tract</th>
      <th>longitude</th>
      <th>latitude</th>
      <th>crime</th>
      <th>residential</th>
      <th>industrial</th>
      <th>river</th>
      <th>nox</th>
      <th>rooms</th>
      <th>older</th>
      <th>distance</th>
      <th>highway</th>
      <th>tax</th>
      <th>ptratio</th>
      <th>lstat</th>
      <th>cmedv</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Nahant</td>
      <td>2011</td>
      <td>-70.955002</td>
      <td>42.255001</td>
      <td>0.00632</td>
      <td>18.0</td>
      <td>2.31</td>
      <td>no</td>
      <td>0.538</td>
      <td>6.575</td>
      <td>65.199997</td>
      <td>4.0900</td>
      <td>1</td>
      <td>296</td>
      <td>15.300000</td>
      <td>4.98</td>
      <td>24.000000</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Swampscott</td>
      <td>2021</td>
      <td>-70.949997</td>
      <td>42.287498</td>
      <td>0.02731</td>
      <td>0.0</td>
      <td>7.07</td>
      <td>no</td>
      <td>0.469</td>
      <td>6.421</td>
      <td>78.900002</td>
      <td>4.9671</td>
      <td>2</td>
      <td>242</td>
      <td>17.799999</td>
      <td>9.14</td>
      <td>21.600000</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Swampscott</td>
      <td>2022</td>
      <td>-70.935997</td>
      <td>42.283001</td>
      <td>0.02729</td>
      <td>0.0</td>
      <td>7.07</td>
      <td>no</td>
      <td>0.469</td>
      <td>7.185</td>
      <td>61.099998</td>
      <td>4.9671</td>
      <td>2</td>
      <td>242</td>
      <td>17.799999</td>
      <td>4.03</td>
      <td>34.700001</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Marblehead</td>
      <td>2031</td>
      <td>-70.928001</td>
      <td>42.292999</td>
      <td>0.03237</td>
      <td>0.0</td>
      <td>2.18</td>
      <td>no</td>
      <td>0.458</td>
      <td>6.998</td>
      <td>45.799999</td>
      <td>6.0622</td>
      <td>3</td>
      <td>222</td>
      <td>18.700001</td>
      <td>2.94</td>
      <td>33.400002</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Marblehead</td>
      <td>2032</td>
      <td>-70.921997</td>
      <td>42.298000</td>
      <td>0.06905</td>
      <td>0.0</td>
      <td>2.18</td>
      <td>no</td>
      <td>0.458</td>
      <td>7.147</td>
      <td>54.200001</td>
      <td>6.0622</td>
      <td>3</td>
      <td>222</td>
      <td>18.700001</td>
      <td>5.33</td>
      <td>36.200001</td>
    </tr>
  </tbody>
</table>
</div>
      <button class="colab-df-convert" onclick="convertToInteractive('df-ef4c7c52-ab5d-49c8-b926-3ba678aca45d')"
              title="Convert this dataframe to an interactive table."
              style="display:none;">
        
  <svg xmlns="http://www.w3.org/2000/svg" height="24px"viewBox="0 0 24 24"
       width="24px">
    <path d="M0 0h24v24H0V0z" fill="none"/>
    <path d="M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z"/><path d="M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z"/>
  </svg>
      </button>
      
  <style>
    .colab-df-container {
      display:flex;
      flex-wrap:wrap;
      gap: 12px;
    }

    .colab-df-convert {
      background-color: #E8F0FE;
      border: none;
      border-radius: 50%;
      cursor: pointer;
      display: none;
      fill: #1967D2;
      height: 32px;
      padding: 0 0 0 0;
      width: 32px;
    }

    .colab-df-convert:hover {
      background-color: #E2EBFA;
      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);
      fill: #174EA6;
    }

    [theme=dark] .colab-df-convert {
      background-color: #3B4455;
      fill: #D2E3FC;
    }

    [theme=dark] .colab-df-convert:hover {
      background-color: #434B5C;
      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);
      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));
      fill: #FFFFFF;
    }
  </style>

      <script>
        const buttonEl =
          document.querySelector('#df-ef4c7c52-ab5d-49c8-b926-3ba678aca45d button.colab-df-convert');
        buttonEl.style.display =
          google.colab.kernel.accessAllowed ? 'block' : 'none';

        async function convertToInteractive(key) {
          const element = document.querySelector('#df-ef4c7c52-ab5d-49c8-b926-3ba678aca45d');
          const dataTable =
            await google.colab.kernel.invokeFunction('convertToInteractive',
                                                     [key], {});
          if (!dataTable) return;

          const docLinkHtml = 'Like what you see? Visit the ' +
            '<a target="_blank" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'
            + ' to learn more about interactive tables.';
          element.innerHTML = '';
          dataTable['output_type'] = 'display_data';
          await google.colab.output.renderOutput(dataTable, element);
          const docLink = document.createElement('div');
          docLink.innerHTML = docLinkHtml;
          element.appendChild(docLink);
        }
      </script>
    </div>
  </div>
  
</div>
</div>
<div class="cell markdown" id="DWTWDD5hP4U2">
<p>For the multivariate analysis, I will choose 3 feature variables to predict the median value of owner occupied homes: average number of rooms per dwelling, per capita crime rate by town, and full-value property tax rate. To get an idea of the immediate linear correlation coefficients of the feature and response variables, I will create a heatmap.</p>
</div>
<div class="cell code" data-colab="{&quot;height&quot;:852,&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}" id="0L_zDpj53pq9" data-outputId="6bcc64dd-3932-4d46-c712-6cf7345aafc2">
<div class="sourceCode" id="cb9"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Select variables and plot data</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> houses[[<span class="st">&#39;rooms&#39;</span>, <span class="st">&#39;crime&#39;</span>, <span class="st">&#39;tax&#39;</span>]].values</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> houses[<span class="st">&#39;cmedv&#39;</span>].values</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Correlation Heatmap</span></span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>irrel_col <span class="op">=</span> [<span class="st">&#39;town&#39;</span>,<span class="st">&#39;tract&#39;</span>,<span class="st">&#39;longitude&#39;</span>,<span class="st">&#39;latitude&#39;</span>,<span class="st">&#39;residential&#39;</span>,<span class="st">&#39;industrial&#39;</span>,<span class="st">&#39;river&#39;</span>,<span class="st">&#39;nox&#39;</span>,<span class="st">&#39;distance&#39;</span>,<span class="st">&#39;older&#39;</span>,<span class="st">&#39;highway&#39;</span>,<span class="st">&#39;ptratio&#39;</span>,<span class="st">&#39;lstat&#39;</span>]</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>houses_select <span class="op">=</span> houses.drop(columns<span class="op">=</span>irrel_col)</span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>c <span class="op">=</span> houses_select.corr()</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>mask <span class="op">=</span> np.triu(c)</span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>,<span class="dv">8</span>))</span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a>sns.heatmap(c, mask<span class="op">=</span>mask, annot<span class="op">=</span><span class="va">True</span>, annot_kws<span class="op">=</span>{<span class="st">&#39;fontsize&#39;</span>:<span class="dv">8</span>,<span class="st">&#39;weight&#39;</span>:<span class="st">&#39;bold&#39;</span>}, cmap<span class="op">=</span><span class="st">&#39;BuPu&#39;</span>, vmin<span class="op">=</span><span class="dv">0</span>, vmax<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a>            yticklabels<span class="op">=</span>[<span class="st">&#39; &#39;</span>] <span class="op">+</span> <span class="bu">list</span>(houses_select.columns[<span class="dv">1</span>:]), xticklabels<span class="op">=</span>houses_select.columns[:<span class="op">-</span><span class="dv">1</span>])</span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a>plt.tick_params(size<span class="op">=</span><span class="dv">0</span>, labelsize<span class="op">=</span><span class="dv">14</span>, pad<span class="op">=</span><span class="dv">20</span>)</span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">&#39;Pairwise Linear Correlations&#39;</span>, fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a>plt.savefig(<span class="st">&#39;houses_heat.png&#39;</span>)</span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div>
<div class="output display_data">
<p><img src="vertopal_de7364266f74463496a9b8fad12d9995/25f9d79aa57e51d123a8b78bb0cc4035ce7994a8.png" width="786" height="835" /></p>
</div>
</div>
<div class="cell markdown" id="2g_4dVJwChRt">
<p>The only feature that seems to hold any significant linear correlation to median house price is the number of rooms. Just to get an idea of the distribution of the variables, I will create a pairplot of the relevant data fields.</p>
</div>
<div class="cell code" data-colab="{&quot;height&quot;:1000,&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}" id="6lOnC5V-3pwN" data-outputId="82222522-5e22-4e9d-cb49-b07d3135fa3b">
<div class="sourceCode" id="cb10"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>,<span class="dv">8</span>))</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>sns.pairplot(houses_select)</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>plt.savefig(<span class="st">&#39;houses_pair.png&#39;</span>)</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div>
<div class="output display_data">
<pre><code>&lt;Figure size 960x960 with 0 Axes&gt;</code></pre>
</div>
<div class="output display_data">
<p><img src="vertopal_de7364266f74463496a9b8fad12d9995/521d5d28703d9daccd0edd091f769f906f0442ae.png" width="1182" height="1180" /></p>
</div>
</div>
<div class="cell code" data-execution_count="6" id="9GB1eum7Ef2h">
<div class="sourceCode" id="cb12"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Standardization</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>ss <span class="op">=</span> StandardScaler()</span></code></pre></div>
</div>
<div class="cell code" data-execution_count="7" id="U7MHMSxdEf5X">
<div class="sourceCode" id="cb13"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Cross Validation</span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>k <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>kf <span class="op">=</span> KFold(n_splits<span class="op">=</span>k, shuffle<span class="op">=</span><span class="va">True</span>, random_state<span class="op">=</span><span class="dv">410</span>)</span></code></pre></div>
</div>
<div class="cell markdown" id="PmBEpZP0CmR9">
<p>To compare against the boosted models, I will be testing an ordinary least-squares linear regression, a decision tree regression, and a random forest regression, as well as an artificial neural network, though I suspect this will perform the poorly.</p>
</div>
<div class="cell code" data-execution_count="8" id="eMt-SfmaKXR0">
<div class="sourceCode" id="cb14"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Least Squares Regression</span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>lsr <span class="op">=</span> LinearRegression()</span></code></pre></div>
</div>
<div class="cell code" data-execution_count="9" id="DdiYS7UpD0Fp">
<div class="sourceCode" id="cb15"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Decision Tree Regression</span></span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>dtr <span class="op">=</span> DecisionTreeRegressor(max_depth<span class="op">=</span><span class="dv">2</span>, random_state<span class="op">=</span><span class="dv">410</span>)</span></code></pre></div>
</div>
<div class="cell code" data-execution_count="10" id="GH3KO88YD-qe">
<div class="sourceCode" id="cb16"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Random Forest Regression</span></span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>rfr <span class="op">=</span> RandomForestRegressor(n_estimators<span class="op">=</span><span class="dv">50</span>, max_depth<span class="op">=</span><span class="dv">2</span>)</span></code></pre></div>
</div>
<div class="cell code" data-execution_count="11" id="fEEiIBmXMFjj">
<div class="sourceCode" id="cb17"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Neural Network</span></span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>seq <span class="op">=</span> Sequential()</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>seq.add(Dense(<span class="dv">32</span>, activation<span class="op">=</span><span class="st">&quot;relu&quot;</span>))</span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a>seq.add(Dense(<span class="dv">64</span>, activation<span class="op">=</span><span class="st">&quot;relu&quot;</span>))</span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a>seq.add(Dense(<span class="dv">128</span>, activation<span class="op">=</span><span class="st">&quot;relu&quot;</span>))</span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a>seq.add(Dense(<span class="dv">1</span>, activation<span class="op">=</span><span class="st">&quot;linear&quot;</span>))</span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a>seq.<span class="bu">compile</span>(loss<span class="op">=</span><span class="st">&#39;mean_squared_error&#39;</span>, optimizer<span class="op">=</span>Adam(learning_rate<span class="op">=</span><span class="fl">1e-2</span>))</span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a>es <span class="op">=</span> EarlyStopping(monitor<span class="op">=</span><span class="st">&#39;val_loss&#39;</span>, mode<span class="op">=</span><span class="st">&#39;min&#39;</span>, verbose<span class="op">=</span><span class="dv">1</span>, patience<span class="op">=</span><span class="dv">800</span>)</span></code></pre></div>
</div>
<div class="cell code" data-execution_count="24" id="jXKBkJ1X3p6A">
<div class="sourceCode" id="cb18"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Locally Weighted Regression</span></span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> lwr(X, y, xnew, kern, tau, intercept, booster<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>    n <span class="op">=</span> <span class="bu">len</span>(X) </span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a>    yest <span class="op">=</span> np.zeros(n)</span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="bu">len</span>(y.shape)<span class="op">==</span><span class="dv">1</span>: </span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a>      y <span class="op">=</span> y.reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)</span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="bu">len</span>(X.shape)<span class="op">==</span><span class="dv">1</span>:</span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true" tabindex="-1"></a>      X <span class="op">=</span> X.reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)</span>
<span id="cb18-11"><a href="#cb18-11" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb18-12"><a href="#cb18-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> intercept:</span>
<span id="cb18-13"><a href="#cb18-13" aria-hidden="true" tabindex="-1"></a>      X1 <span class="op">=</span> np.column_stack([np.ones((<span class="bu">len</span>(X),<span class="dv">1</span>)),X])</span>
<span id="cb18-14"><a href="#cb18-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb18-15"><a href="#cb18-15" aria-hidden="true" tabindex="-1"></a>      X1 <span class="op">=</span> X</span>
<span id="cb18-16"><a href="#cb18-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-17"><a href="#cb18-17" aria-hidden="true" tabindex="-1"></a>    w <span class="op">=</span> np.array([kern((X <span class="op">-</span> X[i])<span class="op">/</span>(<span class="dv">2</span><span class="op">*</span>tau)) <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n)])</span>
<span id="cb18-18"><a href="#cb18-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-19"><a href="#cb18-19" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n):          </span>
<span id="cb18-20"><a href="#cb18-20" aria-hidden="true" tabindex="-1"></a>        W <span class="op">=</span> np.diag(w[:,i])</span>
<span id="cb18-21"><a href="#cb18-21" aria-hidden="true" tabindex="-1"></a>        b <span class="op">=</span> np.transpose(X1).dot(W).dot(y)</span>
<span id="cb18-22"><a href="#cb18-22" aria-hidden="true" tabindex="-1"></a>        A <span class="op">=</span> np.transpose(X1).dot(W).dot(X1)</span>
<span id="cb18-23"><a href="#cb18-23" aria-hidden="true" tabindex="-1"></a>        beta, res, rnk, s <span class="op">=</span> lstsq(A, b)</span>
<span id="cb18-24"><a href="#cb18-24" aria-hidden="true" tabindex="-1"></a>        yest[i] <span class="op">=</span> np.dot(X1[i],beta)</span>
<span id="cb18-25"><a href="#cb18-25" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> X.shape[<span class="dv">1</span>]<span class="op">==</span><span class="dv">1</span>:</span>
<span id="cb18-26"><a href="#cb18-26" aria-hidden="true" tabindex="-1"></a>      f <span class="op">=</span> interp1d(X.flatten(),yest,fill_value<span class="op">=</span><span class="st">&#39;extrapolate&#39;</span>)</span>
<span id="cb18-27"><a href="#cb18-27" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb18-28"><a href="#cb18-28" aria-hidden="true" tabindex="-1"></a>      f <span class="op">=</span> LinearNDInterpolator(X, yest)</span>
<span id="cb18-29"><a href="#cb18-29" aria-hidden="true" tabindex="-1"></a>    output <span class="op">=</span> f(xnew) </span>
<span id="cb18-30"><a href="#cb18-30" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="bu">sum</span>(np.isnan(output))<span class="op">&gt;</span><span class="dv">0</span>:</span>
<span id="cb18-31"><a href="#cb18-31" aria-hidden="true" tabindex="-1"></a>      g <span class="op">=</span> NearestNDInterpolator(X,y.ravel()) </span>
<span id="cb18-32"><a href="#cb18-32" aria-hidden="true" tabindex="-1"></a>      output[np.isnan(output)] <span class="op">=</span> g(xnew[np.isnan(output)])</span>
<span id="cb18-33"><a href="#cb18-33" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> output</span></code></pre></div>
</div>
<section id="locally-weighted-regression-boosting" class="cell markdown" id="C8ElKZMcCvP1">
<h2>Locally Weighted Regression Boosting</h2>
</section>
<div class="cell code" data-execution_count="13" id="0j67GP9f3p8J">
<div class="sourceCode" id="cb19"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Boosted Locally Weighted Regression</span></span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> blwr(X, y, xnew, kern, tau, intercept, boost):</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>  Fx <span class="op">=</span> lwr(X,y,X,kern,tau,intercept)</span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a>  new_y <span class="op">=</span> y <span class="op">-</span> Fx</span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>  boost.fit(X,new_y)</span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a>  output <span class="op">=</span> boost.predict(xnew) <span class="op">+</span> lwr(X,y,xnew,kern,tau,intercept)</span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> output </span></code></pre></div>
</div>
<section id="extreme-gradient-boosting" class="cell markdown" id="Oj_Jh_F0Cvzv">
<h2>Extreme Gradient Boosting</h2>
<p>A derivative of gradient boosting. In this method, a set of parallel decision trees are fitted to the residuals of the previous trees' predictions, so it is an ensemble model similar to random forest. However, instead of relying on traditional decision trees, XGB uses <em>XGBoost trees</em>, which are created by calculating <em>similarity scores</em> between the observations of the leaf nodes. Furthermore, XGB implements regularization to prevent overfitting of the individual decision trees. With XGB, the process of sequentially creating weak models is done so as a <em>gradient descent</em> algorithm using an objective cost function, in which the algorithm iterates until the cost function is at close to or equal to zero, or at least at the lowest minima. In order to minimize the loss function,we take its partial derivative with respect to its slope and its intercept, then subtracting this from the slope beta (denoted theta in the case of the equation below). The partial derivative is scaled by a desired learning rate (hyperparameter) and this process is repeated until a convergence at the minimum.</p>
<p><img src="vertopal_de7364266f74463496a9b8fad12d9995/10255763d97490ec18f218c5d815c13a9dc8d915.png" alt="xgbcostfunctionderivative" /></p>
<p>Source: <a href="https://medium.com/analytics-vidhya/what-makes-xgboost-so-extreme-e1544a4433bb" class="uri">https://medium.com/analytics-vidhya/what-makes-xgboost-so-extreme-e1544a4433bb</a></p>
</section>
<div class="cell code" data-execution_count="14" id="7zCnKQdHLCNc">
<div class="sourceCode" id="cb20"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Extreme Gradient Regression</span></span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>xgbr <span class="op">=</span> XGBRegressor(objective<span class="op">=</span><span class="st">&#39;reg:squarederror&#39;</span>, n_estimators<span class="op">=</span><span class="dv">100</span>, reg_lambda<span class="op">=</span><span class="dv">20</span>, alpha<span class="op">=</span><span class="dv">1</span>, gamma<span class="op">=</span><span class="dv">10</span>, max_depth<span class="op">=</span><span class="dv">3</span>)</span></code></pre></div>
</div>
<div class="cell code" data-execution_count="15" id="k8pWPy0wEf_W">
<div class="sourceCode" id="cb21"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Model Set 1</span></span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Linear (lsr), Decision Tree (dtr), Random Forest (rfr), Extreme Gradient (xgr), </span></span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> RunModel1(model, x , y, scaler, split):</span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a>  mse_model <span class="op">=</span> []</span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a>  mae_model <span class="op">=</span> []</span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a>  r2_model <span class="op">=</span> []</span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> idxtrain, idxtest <span class="kw">in</span> kf.split(x):</span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a>    xtrain <span class="op">=</span> x[idxtrain]</span>
<span id="cb21-10"><a href="#cb21-10" aria-hidden="true" tabindex="-1"></a>    ytrain <span class="op">=</span> y[idxtrain]</span>
<span id="cb21-11"><a href="#cb21-11" aria-hidden="true" tabindex="-1"></a>    ytest <span class="op">=</span> y[idxtest]</span>
<span id="cb21-12"><a href="#cb21-12" aria-hidden="true" tabindex="-1"></a>    xtest <span class="op">=</span> x[idxtest]</span>
<span id="cb21-13"><a href="#cb21-13" aria-hidden="true" tabindex="-1"></a>    xtrain <span class="op">=</span> ss.fit_transform(xtrain)</span>
<span id="cb21-14"><a href="#cb21-14" aria-hidden="true" tabindex="-1"></a>    xtest <span class="op">=</span> ss.transform(xtest)</span>
<span id="cb21-15"><a href="#cb21-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-16"><a href="#cb21-16" aria-hidden="true" tabindex="-1"></a>    model.fit(xtrain, ytrain)</span>
<span id="cb21-17"><a href="#cb21-17" aria-hidden="true" tabindex="-1"></a>    yhat <span class="op">=</span> model.predict(xtest)</span>
<span id="cb21-18"><a href="#cb21-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-19"><a href="#cb21-19" aria-hidden="true" tabindex="-1"></a>    mse_model.append(mse(ytest, yhat))</span>
<span id="cb21-20"><a href="#cb21-20" aria-hidden="true" tabindex="-1"></a>    mae_model.append(mae(ytest, yhat))</span>
<span id="cb21-21"><a href="#cb21-21" aria-hidden="true" tabindex="-1"></a>    r2_model.append(r2(ytest, yhat))</span>
<span id="cb21-22"><a href="#cb21-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-23"><a href="#cb21-23" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> np.mean(mse_model), np.mean(mae_model), np.mean(r2_model)</span></code></pre></div>
</div>
<div class="cell code" data-execution_count="34" id="wsXVETnDIylR">
<div class="sourceCode" id="cb22"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Model Set 2</span></span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Loess, Boosted Loess</span></span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> RunModel2(model, x , y, kern, tau, scaler, split, booster<span class="op">=</span><span class="va">None</span>, n_boosts<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a>  mse_model <span class="op">=</span> []</span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a>  mae_model <span class="op">=</span> []</span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a>  r2_model <span class="op">=</span> []</span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> idxtrain, idxtest <span class="kw">in</span> kf.split(x):</span>
<span id="cb22-9"><a href="#cb22-9" aria-hidden="true" tabindex="-1"></a>    xtrain <span class="op">=</span> x[idxtrain]</span>
<span id="cb22-10"><a href="#cb22-10" aria-hidden="true" tabindex="-1"></a>    ytrain <span class="op">=</span> y[idxtrain]</span>
<span id="cb22-11"><a href="#cb22-11" aria-hidden="true" tabindex="-1"></a>    ytest <span class="op">=</span> y[idxtest]</span>
<span id="cb22-12"><a href="#cb22-12" aria-hidden="true" tabindex="-1"></a>    xtest <span class="op">=</span> x[idxtest]</span>
<span id="cb22-13"><a href="#cb22-13" aria-hidden="true" tabindex="-1"></a>    xtrain <span class="op">=</span> ss.fit_transform(xtrain)</span>
<span id="cb22-14"><a href="#cb22-14" aria-hidden="true" tabindex="-1"></a>    xtest <span class="op">=</span> ss.transform(xtest)</span>
<span id="cb22-15"><a href="#cb22-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-16"><a href="#cb22-16" aria-hidden="true" tabindex="-1"></a>    yhat <span class="op">=</span> model(xtrain, ytrain, xtest, kern, tau<span class="op">=</span>tau, intercept<span class="op">=</span><span class="va">True</span>, booster<span class="op">=</span>booster, n_boosts<span class="op">=</span>n_boosts)</span>
<span id="cb22-17"><a href="#cb22-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-18"><a href="#cb22-18" aria-hidden="true" tabindex="-1"></a>    mse_model.append(mse(ytest, yhat))</span>
<span id="cb22-19"><a href="#cb22-19" aria-hidden="true" tabindex="-1"></a>    mae_model.append(mae(ytest, yhat))</span>
<span id="cb22-20"><a href="#cb22-20" aria-hidden="true" tabindex="-1"></a>    r2_model.append(r2(ytest, yhat))</span>
<span id="cb22-21"><a href="#cb22-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-22"><a href="#cb22-22" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> np.mean(mse_model), np.mean(mae_model), np.mean(r2_model)</span></code></pre></div>
</div>
<div class="cell code" data-execution_count="18" id="09vAua6eIyoE">
<div class="sourceCode" id="cb23"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Model Set 3</span></span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Neural Network</span></span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> RunNN(model, x , y, scaler, split, val_split<span class="op">=</span><span class="fl">0.25</span>, epoch<span class="op">=</span><span class="dv">500</span>, batch<span class="op">=</span><span class="dv">20</span>):</span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a>  mse_model <span class="op">=</span> []</span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a>  mae_model <span class="op">=</span> []</span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true" tabindex="-1"></a>  r2_model <span class="op">=</span> []</span>
<span id="cb23-7"><a href="#cb23-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-8"><a href="#cb23-8" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> idxtrain, idxtest <span class="kw">in</span> kf.split(x):</span>
<span id="cb23-9"><a href="#cb23-9" aria-hidden="true" tabindex="-1"></a>    xtrain <span class="op">=</span> x[idxtrain]</span>
<span id="cb23-10"><a href="#cb23-10" aria-hidden="true" tabindex="-1"></a>    ytrain <span class="op">=</span> y[idxtrain]</span>
<span id="cb23-11"><a href="#cb23-11" aria-hidden="true" tabindex="-1"></a>    ytest <span class="op">=</span> y[idxtest]</span>
<span id="cb23-12"><a href="#cb23-12" aria-hidden="true" tabindex="-1"></a>    xtest <span class="op">=</span> x[idxtest]</span>
<span id="cb23-13"><a href="#cb23-13" aria-hidden="true" tabindex="-1"></a>    xtrain <span class="op">=</span> ss.fit_transform(xtrain)</span>
<span id="cb23-14"><a href="#cb23-14" aria-hidden="true" tabindex="-1"></a>    xtest <span class="op">=</span> ss.transform(xtest)</span>
<span id="cb23-15"><a href="#cb23-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-16"><a href="#cb23-16" aria-hidden="true" tabindex="-1"></a>    model.fit(xtrain, ytrain, validation_split<span class="op">=</span>val_split, epochs<span class="op">=</span>epoch, batch_size<span class="op">=</span>batch, verbose<span class="op">=</span><span class="dv">0</span>, callbacks<span class="op">=</span>[es])</span>
<span id="cb23-17"><a href="#cb23-17" aria-hidden="true" tabindex="-1"></a>    yhat <span class="op">=</span> seq.predict(xtest)</span>
<span id="cb23-18"><a href="#cb23-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-19"><a href="#cb23-19" aria-hidden="true" tabindex="-1"></a>    mse_model.append(mse(ytest, yhat))</span>
<span id="cb23-20"><a href="#cb23-20" aria-hidden="true" tabindex="-1"></a>    mae_model.append(mae(ytest, yhat))</span>
<span id="cb23-21"><a href="#cb23-21" aria-hidden="true" tabindex="-1"></a>    r2_model.append(r2(ytest, yhat))</span>
<span id="cb23-22"><a href="#cb23-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-23"><a href="#cb23-23" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> np.mean(mse_model), np.mean(mae_model), np.mean(r2_model)</span></code></pre></div>
</div>
<div class="cell code" data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}" id="un1_Q_sGIyxu" data-outputId="60e454c1-1d2f-4e37-deb3-3655276a199a">
<div class="sourceCode" id="cb24"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a>mse_lsr, mae_lsr, r2_lsr <span class="op">=</span> RunModel1(lsr, x, y, ss, kf)</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;The Cross-validated Mean Squared Error for Least-Squares Regression is : &#39;</span> <span class="op">+</span> <span class="bu">str</span>(mse_lsr))</span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;The Cross-validated Mean Absolute Error for Least-Squares Regression is : &#39;</span> <span class="op">+</span> <span class="bu">str</span>(mae_lsr))</span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;The Cross-validated Coefficient of Determination for Least-Squares Regression is : &#39;</span> <span class="op">+</span> <span class="bu">str</span>(r2_lsr))</span></code></pre></div>
<div class="output stream stdout">
<pre><code>The Cross-validated Mean Squared Error for Least-Squares Regression is : 37.23940104890824
The Cross-validated Mean Absolute Error for Least-Squares Regression is : 4.020970004960848
The Cross-validated Coefficient of Determination for Least-Squares Regression is : 0.5478240461818417
</code></pre>
</div>
</div>
<div class="cell code" data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}" id="bNIxecxSPvis" data-outputId="a757e604-4228-48b4-9c56-4423cb297ae5">
<div class="sourceCode" id="cb26"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a>mse_dtr, mae_dtr, r2_dtr <span class="op">=</span> RunModel1(dtr, x, y, ss, kf)</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;The Cross-validated Mean Squared Error for Decision Tree Regression is : &#39;</span> <span class="op">+</span> <span class="bu">str</span>(mse_dtr))</span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;The Cross-validated Mean Absolute Error for Decision Tree Regression is : &#39;</span> <span class="op">+</span> <span class="bu">str</span>(mae_dtr))</span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;The Cross-validated Coefficient of Determination for Decision Tree Regression is : &#39;</span> <span class="op">+</span> <span class="bu">str</span>(r2_dtr))</span></code></pre></div>
<div class="output stream stdout">
<pre><code>The Cross-validated Mean Squared Error for Decision Tree Regression is : 35.18217909437036
The Cross-validated Mean Absolute Error for Decision Tree Regression is : 4.063618925443233
The Cross-validated Coefficient of Determination for Decision Tree Regression is : 0.5680960234893112
</code></pre>
</div>
</div>
<div class="cell code" data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}" id="9vbMtQ-gPvlO" data-outputId="2e21e9e9-b28e-46c9-8fd6-aa5abe51d941">
<div class="sourceCode" id="cb28"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a>mse_rfr, mae_rfr, r2_rfr <span class="op">=</span> RunModel1(rfr, x, y, ss, kf)</span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;The Cross-validated Mean Squared Error for Random Forest Regression is : &#39;</span> <span class="op">+</span> <span class="bu">str</span>(mse_rfr))</span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;The Cross-validated Mean Absolute Error for Random Forest Regression is : &#39;</span> <span class="op">+</span> <span class="bu">str</span>(mae_rfr))</span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;The Cross-validated Coefficient of Determination for Random Forest Regression is : &#39;</span> <span class="op">+</span> <span class="bu">str</span>(r2_rfr))</span></code></pre></div>
<div class="output stream stdout">
<pre><code>The Cross-validated Mean Squared Error for Random Forest Regression is : 31.9639600875443
The Cross-validated Mean Absolute Error for Random Forest Regression is : 3.8578840078128542
The Cross-validated Coefficient of Determination for Random Forest Regression is : 0.6121888076614384
</code></pre>
</div>
</div>
<div class="cell code" data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}" id="Ey9U_yFtadp2" data-outputId="3bd8e492-0997-46db-e11a-9a2274a8c2ec">
<div class="sourceCode" id="cb30"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a>mse_seq, mae_seq, r2_seq <span class="op">=</span> RunNN(seq, x, y, ss, kf)</span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;The Cross-validated Mean Squared Error for Neural Network is : &#39;</span> <span class="op">+</span> <span class="bu">str</span>(mse_seq))</span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;The Cross-validated Mean Absolute Error for Neural Network is : &#39;</span> <span class="op">+</span> <span class="bu">str</span>(mae_seq))</span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;The Cross-validated Coefficient of Determination for Neural Network is : &#39;</span> <span class="op">+</span> <span class="bu">str</span>(r2_seq))</span></code></pre></div>
<div class="output stream stdout">
<pre><code>The Cross-validated Mean Squared Error for Neural Network is : 85.57445466742146
The Cross-validated Mean Absolute Error for Neural Network is : 5.030748784209109
The Cross-validated Coefficient of Determination for Neural Network is : -0.1203951433644184
</code></pre>
</div>
</div>
<div class="cell code" data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}" id="Bpwd9uFhPvqL" data-outputId="dc1a2bfc-6ca0-473f-8a65-a342bbea9659">
<div class="sourceCode" id="cb32"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a>mse_lwr, mae_lwr, r2_lwr <span class="op">=</span> RunModel2(lwr, x, y, Tricubic, <span class="fl">0.9</span>, ss, kf)</span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;The Cross-validated Mean Squared Error for Locally Weighted Regression is : &#39;</span> <span class="op">+</span> <span class="bu">str</span>(mse_lwr))</span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;The Cross-validated Mean Absolute Error for Locally Weighted Regression is : &#39;</span> <span class="op">+</span> <span class="bu">str</span>(mae_lwr))</span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;The Cross-validated Coefficient of Determination for Locally Weighted Regression is : &#39;</span> <span class="op">+</span> <span class="bu">str</span>(r2_lwr))</span></code></pre></div>
<div class="output stream stdout">
<pre><code>The Cross-validated Mean Squared Error for Locally Weighted Regression is : 26.597270697712815
The Cross-validated Mean Absolute Error for Locally Weighted Regression is : 3.3010239544041284
The Cross-validated Coefficient of Determination for Locally Weighted Regression is : 0.6845775749624277
</code></pre>
</div>
</div>
<div class="cell code" data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}" id="yEPe6LkAZOu3" data-outputId="68f655f6-c0be-4c24-ce72-5cc592bb1a4d">
<div class="sourceCode" id="cb34"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a>mse_blwr, mae_blwr, r2_blwr <span class="op">=</span> RunModel2(blwr, x, y, Quartic, <span class="fl">0.9</span>, ss, kf, dtr)</span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;The Cross-validated Mean Squared Error for Locally Weighted Regression Boosted with Decision Trees is : &#39;</span> <span class="op">+</span> <span class="bu">str</span>(mse_blwr))</span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;The Cross-validated Mean Absolute Error for Locally Weighted Regression Boosted with Decision Trees is : &#39;</span> <span class="op">+</span> <span class="bu">str</span>(mae_blwr))</span>
<span id="cb34-4"><a href="#cb34-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;The Cross-validated Coefficient of Determination for Locally Weighted Regression Boosted with Decision Trees is : &#39;</span> <span class="op">+</span> <span class="bu">str</span>(r2_blwr))</span></code></pre></div>
<div class="output stream stdout">
<pre><code>The Cross-validated Mean Squared Error for Locally Weighted Regression Boosted with Decision Trees is : 26.943911216871744
The Cross-validated Mean Absolute Error for Locally Weighted Regression Boosted with Decision Trees is : 3.2946931437035536
The Cross-validated Coefficient of Determination for Locally Weighted Regression Boosted with Decision Trees is : 0.6813133952301352
</code></pre>
</div>
</div>
<div class="cell code" data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}" id="YUuFpIi-Pvsk" data-outputId="b89387de-a721-4913-dab4-3fe89724c228">
<div class="sourceCode" id="cb36"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a>mse_blwr, mae_blwr, r2_blwr <span class="op">=</span> RunModel2(blwr, x, y, Quartic, <span class="fl">0.9</span>, ss, kf, boost<span class="op">=</span>rfr)</span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;The Cross-validated Mean Squared Error for Locally Weighted Regression Boosted with Random Forest is : &#39;</span> <span class="op">+</span> <span class="bu">str</span>(mse_blwr))</span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;The Cross-validated Mean Absolute Error for Locally Weighted Regression Boosted with Random Forest is : &#39;</span> <span class="op">+</span> <span class="bu">str</span>(mae_blwr))</span>
<span id="cb36-4"><a href="#cb36-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;The Cross-validated Coefficient of Determination for Locally Weighted Regression Boosted with Random Forest is : &#39;</span> <span class="op">+</span> <span class="bu">str</span>(r2_blwr))</span></code></pre></div>
<div class="output stream stdout">
<pre><code>The Cross-validated Mean Squared Error for Locally Weighted Regression Boosted with Random Forest is : 26.68176106594833
The Cross-validated Mean Absolute Error for Locally Weighted Regression Boosted with Random Forest is : 3.2491331849574685
The Cross-validated Coefficient of Determination for Locally Weighted Regression Boosted with Random Forest is : 0.6858306843421442
</code></pre>
</div>
</div>
<div class="cell code" data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}" id="eK2wCvQhPvnl" data-outputId="317dfb2c-01e9-4a94-eb15-755bfeab0aab">
<div class="sourceCode" id="cb38"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a>mse_xgbr, mae_xgbr, r2_xgbr <span class="op">=</span> RunModel1(xgbr, x, y, ss, kf)</span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;The Cross-validated Mean Squared Error for Extreme Gradient Regression is : &#39;</span> <span class="op">+</span> <span class="bu">str</span>(mse_xgbr))</span>
<span id="cb38-3"><a href="#cb38-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;The Cross-validated Mean Absolute Error for Extreme Gradient Regression is : &#39;</span> <span class="op">+</span> <span class="bu">str</span>(mae_xgbr))</span>
<span id="cb38-4"><a href="#cb38-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;The Cross-validated Coefficient of Determination for Extreme Gradient Regression is : &#39;</span> <span class="op">+</span> <span class="bu">str</span>(r2_xgbr))</span></code></pre></div>
<div class="output stream stdout">
<pre><code>The Cross-validated Mean Squared Error for Extreme Gradient Regression is : 26.548227007953603
The Cross-validated Mean Absolute Error for Extreme Gradient Regression is : 3.217740893794725
The Cross-validated Coefficient of Determination for Extreme Gradient Regression is : 0.6886189666427519
</code></pre>
</div>
</div>
<div class="cell markdown" id="QsI5V3DfC0Ol">
<p>Overall, all of the models performed relatively poorly. Locally weighted regression performed the best out of the standard nonparametric models, performing marginally better than its primary competitor, random forest regression. On a side note, the ordinary linear regression did not fare so badly against the decision tree regression, which was a suprising result. The artificial neural network is a useless model and is unable to account for any of the explained variation in the data with a negative R^2 score. Strangely, the boosted versions of the locally weighted regression, both the decision tree and random forest, performed slightly poorer than the standard model, which is to say the current state that I constructed added no predicitve power. Unfortunately, the same can be said for the extreme gradient boosting, which performed the best out of all of the models. Overall, the boosted models did perform marginally better than the standard models; however, I suspect that they are ill-equipped to make solid predictions for this particular data.</p>
</div>
<section id="concrete-data" class="cell markdown" id="Kki_Q2pCC0UR">
<h3>Concrete Data</h3>
<p>The second dataset I will try to model will be the Boston Housing Prices dataset, which can be found here: <a href="https://www.kaggle.com/elikplim/concrete-compressive-strength-data-set" class="uri">https://www.kaggle.com/elikplim/concrete-compressive-strength-data-set</a></p>
</section>
<div class="cell code" data-execution_count="19" data-colab="{&quot;height&quot;:206,&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}" id="ghJGk4vW3qBn" data-outputId="0f1f53f5-d733-4c59-8233-b715dd0da464">
<div class="sourceCode" id="cb40"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Load Data</span></span>
<span id="cb40-2"><a href="#cb40-2" aria-hidden="true" tabindex="-1"></a>concrete <span class="op">=</span> pd.read_csv(<span class="st">&#39;drive/MyDrive/Data/concrete.csv&#39;</span>)</span>
<span id="cb40-3"><a href="#cb40-3" aria-hidden="true" tabindex="-1"></a>concrete.head()</span></code></pre></div>
<div class="output execute_result" data-execution_count="19">

  <div id="df-1ce74ba3-eeb8-4086-a23c-24947ebf2d2b">
    <div class="colab-df-container">
      <div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>cement</th>
      <th>slag</th>
      <th>ash</th>
      <th>water</th>
      <th>superplastic</th>
      <th>coarseagg</th>
      <th>fineagg</th>
      <th>age</th>
      <th>strength</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>540.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>162.0</td>
      <td>2.5</td>
      <td>1040.0</td>
      <td>676.0</td>
      <td>28</td>
      <td>79.99</td>
    </tr>
    <tr>
      <th>1</th>
      <td>540.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>162.0</td>
      <td>2.5</td>
      <td>1055.0</td>
      <td>676.0</td>
      <td>28</td>
      <td>61.89</td>
    </tr>
    <tr>
      <th>2</th>
      <td>332.5</td>
      <td>142.5</td>
      <td>0.0</td>
      <td>228.0</td>
      <td>0.0</td>
      <td>932.0</td>
      <td>594.0</td>
      <td>270</td>
      <td>40.27</td>
    </tr>
    <tr>
      <th>3</th>
      <td>332.5</td>
      <td>142.5</td>
      <td>0.0</td>
      <td>228.0</td>
      <td>0.0</td>
      <td>932.0</td>
      <td>594.0</td>
      <td>365</td>
      <td>41.05</td>
    </tr>
    <tr>
      <th>4</th>
      <td>198.6</td>
      <td>132.4</td>
      <td>0.0</td>
      <td>192.0</td>
      <td>0.0</td>
      <td>978.4</td>
      <td>825.5</td>
      <td>360</td>
      <td>44.30</td>
    </tr>
  </tbody>
</table>
</div>
      <button class="colab-df-convert" onclick="convertToInteractive('df-1ce74ba3-eeb8-4086-a23c-24947ebf2d2b')"
              title="Convert this dataframe to an interactive table."
              style="display:none;">
        
  <svg xmlns="http://www.w3.org/2000/svg" height="24px"viewBox="0 0 24 24"
       width="24px">
    <path d="M0 0h24v24H0V0z" fill="none"/>
    <path d="M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z"/><path d="M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z"/>
  </svg>
      </button>
      
  <style>
    .colab-df-container {
      display:flex;
      flex-wrap:wrap;
      gap: 12px;
    }

    .colab-df-convert {
      background-color: #E8F0FE;
      border: none;
      border-radius: 50%;
      cursor: pointer;
      display: none;
      fill: #1967D2;
      height: 32px;
      padding: 0 0 0 0;
      width: 32px;
    }

    .colab-df-convert:hover {
      background-color: #E2EBFA;
      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);
      fill: #174EA6;
    }

    [theme=dark] .colab-df-convert {
      background-color: #3B4455;
      fill: #D2E3FC;
    }

    [theme=dark] .colab-df-convert:hover {
      background-color: #434B5C;
      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);
      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));
      fill: #FFFFFF;
    }
  </style>

      <script>
        const buttonEl =
          document.querySelector('#df-1ce74ba3-eeb8-4086-a23c-24947ebf2d2b button.colab-df-convert');
        buttonEl.style.display =
          google.colab.kernel.accessAllowed ? 'block' : 'none';

        async function convertToInteractive(key) {
          const element = document.querySelector('#df-1ce74ba3-eeb8-4086-a23c-24947ebf2d2b');
          const dataTable =
            await google.colab.kernel.invokeFunction('convertToInteractive',
                                                     [key], {});
          if (!dataTable) return;

          const docLinkHtml = 'Like what you see? Visit the ' +
            '<a target="_blank" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'
            + ' to learn more about interactive tables.';
          element.innerHTML = '';
          dataTable['output_type'] = 'display_data';
          await google.colab.output.renderOutput(dataTable, element);
          const docLink = document.createElement('div');
          docLink.innerHTML = docLinkHtml;
          element.appendChild(docLink);
        }
      </script>
    </div>
  </div>
  
</div>
</div>
<div class="cell markdown" id="XkpeOfyLR5YI">
<p>Like before, I will choose 3 feature variables to predict the compressive strength of the concrete: the amount of the cement component, the amount of the slag component, and the amount of the water component. To get an idea of the immediate linear correlation coefficients and distributions of the feature and response variables, I will again create a heatmap and a pairplot.</p>
</div>
<div class="cell code" data-execution_count="20" data-colab="{&quot;height&quot;:852,&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}" id="RDvh62G9IWX8" data-outputId="c2bd1e4a-8d65-43f2-eed4-453f8b4ba4f6">
<div class="sourceCode" id="cb41"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Select variables and plot data</span></span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> concrete[[<span class="st">&#39;cement&#39;</span>,<span class="st">&#39;slag&#39;</span>,<span class="st">&#39;water&#39;</span>]].values</span>
<span id="cb41-3"><a href="#cb41-3" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> concrete[<span class="st">&#39;strength&#39;</span>].values</span>
<span id="cb41-4"><a href="#cb41-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-5"><a href="#cb41-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Correlation Heatmap</span></span>
<span id="cb41-6"><a href="#cb41-6" aria-hidden="true" tabindex="-1"></a>irrel_col <span class="op">=</span> [<span class="st">&#39;ash&#39;</span>,<span class="st">&#39;superplastic&#39;</span>,<span class="st">&#39;coarseagg&#39;</span>,<span class="st">&#39;fineagg&#39;</span>,<span class="st">&#39;age&#39;</span>]</span>
<span id="cb41-7"><a href="#cb41-7" aria-hidden="true" tabindex="-1"></a>concrete_select <span class="op">=</span> concrete.drop(columns<span class="op">=</span>irrel_col)</span>
<span id="cb41-8"><a href="#cb41-8" aria-hidden="true" tabindex="-1"></a>c <span class="op">=</span> concrete_select.corr()</span>
<span id="cb41-9"><a href="#cb41-9" aria-hidden="true" tabindex="-1"></a>mask <span class="op">=</span> np.triu(c)</span>
<span id="cb41-10"><a href="#cb41-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-11"><a href="#cb41-11" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>,<span class="dv">8</span>))</span>
<span id="cb41-12"><a href="#cb41-12" aria-hidden="true" tabindex="-1"></a>sns.heatmap(c, mask<span class="op">=</span>mask, annot<span class="op">=</span><span class="va">True</span>, annot_kws<span class="op">=</span>{<span class="st">&#39;fontsize&#39;</span>:<span class="dv">8</span>,<span class="st">&#39;weight&#39;</span>:<span class="st">&#39;bold&#39;</span>}, cmap<span class="op">=</span><span class="st">&#39;PuBu&#39;</span>, vmin<span class="op">=</span><span class="dv">0</span>, vmax<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb41-13"><a href="#cb41-13" aria-hidden="true" tabindex="-1"></a>            yticklabels<span class="op">=</span>[<span class="st">&#39; &#39;</span>] <span class="op">+</span> <span class="bu">list</span>(concrete_select.columns[<span class="dv">1</span>:]), xticklabels<span class="op">=</span>concrete_select.columns[:<span class="op">-</span><span class="dv">1</span>])</span>
<span id="cb41-14"><a href="#cb41-14" aria-hidden="true" tabindex="-1"></a>plt.tick_params(size<span class="op">=</span><span class="dv">0</span>, labelsize<span class="op">=</span><span class="dv">14</span>, pad<span class="op">=</span><span class="dv">20</span>)</span>
<span id="cb41-15"><a href="#cb41-15" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">&#39;Pairwise Linear Correlations&#39;</span>, fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb41-16"><a href="#cb41-16" aria-hidden="true" tabindex="-1"></a>plt.savefig(<span class="st">&#39;concrete_heat.png&#39;</span>)</span>
<span id="cb41-17"><a href="#cb41-17" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div>
<div class="output display_data">
<p><img src="vertopal_de7364266f74463496a9b8fad12d9995/bf69307732d65a8a18d3aa7c936045d2d0a6becd.png" width="786" height="835" /></p>
</div>
</div>
<div class="cell code" data-colab="{&quot;height&quot;:1000,&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}" id="1Uoxiv0E-6TU" data-outputId="c56ea2d0-e91f-4d98-c878-598af5a105b0">
<div class="sourceCode" id="cb42"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>,<span class="dv">8</span>))</span>
<span id="cb42-2"><a href="#cb42-2" aria-hidden="true" tabindex="-1"></a>sns.pairplot(concrete_select)</span>
<span id="cb42-3"><a href="#cb42-3" aria-hidden="true" tabindex="-1"></a>plt.savefig(<span class="st">&#39;concrete_pair.png&#39;</span>)</span>
<span id="cb42-4"><a href="#cb42-4" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div>
<div class="output display_data">
<pre><code>&lt;Figure size 960x960 with 0 Axes&gt;</code></pre>
</div>
<div class="output display_data">
<p><img src="vertopal_de7364266f74463496a9b8fad12d9995/a75ef388cf021512a6a8a68f9fbd2976abc0a765.png" width="1182" height="1180" /></p>
</div>
</div>
<div class="cell code" data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}" id="G0jbXPPlPU3E" data-outputId="8949a66d-1508-4567-9280-df6c62b17b05">
<div class="sourceCode" id="cb44"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a>mse_lsr, mae_lsr, r2_lsr <span class="op">=</span> RunModel1(lsr, x, y, ss, kf)</span>
<span id="cb44-2"><a href="#cb44-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;The Cross-validated Mean Squared Error for Least-Squares Regression is : &#39;</span> <span class="op">+</span> <span class="bu">str</span>(mse_lsr))</span>
<span id="cb44-3"><a href="#cb44-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;The Cross-validated Mean Absolute Error for Least-Squares Regression is : &#39;</span> <span class="op">+</span> <span class="bu">str</span>(mae_lsr))</span>
<span id="cb44-4"><a href="#cb44-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;The Cross-validated Coefficient of Determination for Least-Squares Regression is : &#39;</span> <span class="op">+</span> <span class="bu">str</span>(r2_lsr))</span></code></pre></div>
<div class="output stream stdout">
<pre><code>The Cross-validated Mean Squared Error for Least-Squares Regression is : 167.78526376603222
The Cross-validated Mean Absolute Error for Least-Squares Regression is : 10.429406509600094
The Cross-validated Coefficient of Determination for Least-Squares Regression is : 0.38806908236831456
</code></pre>
</div>
</div>
<div class="cell code" data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}" id="hxSbTvXeAMQx" data-outputId="080e0791-4712-4559-a107-f2f1ef80989b">
<div class="sourceCode" id="cb46"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a>mse_dtr, mae_dtr, r2_dtr <span class="op">=</span> RunModel1(dtr, x, y, ss, kf)</span>
<span id="cb46-2"><a href="#cb46-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;The Cross-validated Mean Squared Error for Decision Tree Regression is : &#39;</span> <span class="op">+</span> <span class="bu">str</span>(mse_dtr))</span>
<span id="cb46-3"><a href="#cb46-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;The Cross-validated Mean Absolute Error for Decision Tree Regression is : &#39;</span> <span class="op">+</span> <span class="bu">str</span>(mae_dtr))</span>
<span id="cb46-4"><a href="#cb46-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;The Cross-validated Coefficient of Determination for Decision Tree Regression is : &#39;</span> <span class="op">+</span> <span class="bu">str</span>(r2_dtr))</span></code></pre></div>
<div class="output stream stdout">
<pre><code>The Cross-validated Mean Squared Error for Decision Tree Regression is : 196.55510778069464
The Cross-validated Mean Absolute Error for Decision Tree Regression is : 11.42242710516928
The Cross-validated Coefficient of Determination for Decision Tree Regression is : 0.283131231488064
</code></pre>
</div>
</div>
<div class="cell code" data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}" id="juXMgjHMAQ4B" data-outputId="c2b29868-6a22-4fcf-8f23-9ce112d4ea6f">
<div class="sourceCode" id="cb48"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a>mse_rfr, mae_rfr, r2_rfr <span class="op">=</span> RunModel1(rfr, x, y, ss, kf)</span>
<span id="cb48-2"><a href="#cb48-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;The Cross-validated Mean Squared Error for Random Forest Regression is : &#39;</span> <span class="op">+</span> <span class="bu">str</span>(mse_rfr))</span>
<span id="cb48-3"><a href="#cb48-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;The Cross-validated Mean Absolute Error for Random Forest Regression is : &#39;</span> <span class="op">+</span> <span class="bu">str</span>(mae_rfr))</span>
<span id="cb48-4"><a href="#cb48-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;The Cross-validated Coefficient of Determination for Random Forest Regression is : &#39;</span> <span class="op">+</span> <span class="bu">str</span>(r2_rfr))</span></code></pre></div>
<div class="output stream stdout">
<pre><code>The Cross-validated Mean Squared Error for Random Forest Regression is : 185.20401382173023
The Cross-validated Mean Absolute Error for Random Forest Regression is : 11.121562873963494
The Cross-validated Coefficient of Determination for Random Forest Regression is : 0.3260469998620026
</code></pre>
</div>
</div>
<div class="cell code" data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}" id="iz1VrCrZCWEY" data-outputId="968d205c-ab7e-42c3-961a-d710db50f77e">
<div class="sourceCode" id="cb50"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a>mse_seq, mae_seq, r2_seq <span class="op">=</span> RunNN(seq, x, y, ss, kf)</span>
<span id="cb50-2"><a href="#cb50-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;The Cross-validated Mean Squared Error for Neural Network is : &#39;</span> <span class="op">+</span> <span class="bu">str</span>(mse_seq))</span>
<span id="cb50-3"><a href="#cb50-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;The Cross-validated Mean Absolute Error for Neural Network is : &#39;</span> <span class="op">+</span> <span class="bu">str</span>(mae_seq))</span>
<span id="cb50-4"><a href="#cb50-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;The Cross-validated Coefficient of Determination for Neural Network is : &#39;</span> <span class="op">+</span> <span class="bu">str</span>(r2_seq))</span></code></pre></div>
<div class="output stream stdout">
<pre><code>The Cross-validated Mean Squared Error for Neural Network is : 185.4012886376533
The Cross-validated Mean Absolute Error for Neural Network is : 11.223063258865505
The Cross-validated Coefficient of Determination for Neural Network is : 0.32458313308094616
</code></pre>
</div>
</div>
<div class="cell code" data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}" id="ELKlnX2wBRtM" data-outputId="e8923e21-6f0a-4730-ce26-e11831cc2c52">
<div class="sourceCode" id="cb52"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true" tabindex="-1"></a>mse_lwr, mae_lwr, r2_lwr <span class="op">=</span> RunModel2(lwr, x, y, Tricubic, <span class="fl">0.9</span>, ss, kf)</span>
<span id="cb52-2"><a href="#cb52-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;The Cross-validated Mean Squared Error for Locally Weighted Regression is : &#39;</span> <span class="op">+</span> <span class="bu">str</span>(mse_lwr))</span>
<span id="cb52-3"><a href="#cb52-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;The Cross-validated Mean Absolute Error for Locally Weighted Regression is : &#39;</span> <span class="op">+</span> <span class="bu">str</span>(mae_lwr))</span>
<span id="cb52-4"><a href="#cb52-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;The Cross-validated Coefficient of Determination for Locally Weighted Regression is : &#39;</span> <span class="op">+</span> <span class="bu">str</span>(r2_lwr))</span></code></pre></div>
<div class="output stream stdout">
<pre><code>The Cross-validated Mean Squared Error for Locally Weighted Regression is : 146.4227649591804
The Cross-validated Mean Absolute Error for Locally Weighted Regression is : 9.768092527277217
The Cross-validated Coefficient of Determination for Locally Weighted Regression is : 0.4654909181670496
</code></pre>
</div>
</div>
<div class="cell code" data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}" id="PJ0ObzztCEqZ" data-outputId="0f450be6-18b8-439b-f2d2-f564ddc9bb7b">
<div class="sourceCode" id="cb54"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb54-1"><a href="#cb54-1" aria-hidden="true" tabindex="-1"></a>mse_blwr, mae_blwr, r2_blwr <span class="op">=</span> RunModel2(blwr, x, y, Quartic, <span class="fl">0.9</span>, ss, kf, dtr)</span>
<span id="cb54-2"><a href="#cb54-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;The Cross-validated Mean Squared Error for Locally Weighted Regression Boosted with Decision Trees is : &#39;</span> <span class="op">+</span> <span class="bu">str</span>(mse_blwr))</span>
<span id="cb54-3"><a href="#cb54-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;The Cross-validated Mean Absolute Error for Locally Weighted Regression Boosted with Decision Trees is : &#39;</span> <span class="op">+</span> <span class="bu">str</span>(mae_blwr))</span>
<span id="cb54-4"><a href="#cb54-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;The Cross-validated Coefficient of Determination for Locally Weighted Regression Boosted with Decision Trees is : &#39;</span> <span class="op">+</span> <span class="bu">str</span>(r2_blwr))</span></code></pre></div>
<div class="output stream stdout">
<pre><code>The Cross-validated Mean Squared Error for Locally Weighted Regression Boosted with Decision Trees is : 141.5780947490901
The Cross-validated Mean Absolute Error for Locally Weighted Regression Boosted with Decision Trees is : 9.551662686619025
The Cross-validated Coefficient of Determination for Locally Weighted Regression Boosted with Decision Trees is : 0.4829019098259071
</code></pre>
</div>
</div>
<div class="cell code" data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}" id="B3tzVepMBqhN" data-outputId="8d994d76-8c69-4b27-a257-6d505d43feab">
<div class="sourceCode" id="cb56"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb56-1"><a href="#cb56-1" aria-hidden="true" tabindex="-1"></a>mse_blwr, mae_blwr, r2_blwr <span class="op">=</span> RunModel2(blwr, x, y, Quartic, <span class="fl">0.9</span>, ss, kf, boost<span class="op">=</span>rfr)</span>
<span id="cb56-2"><a href="#cb56-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;The Cross-validated Mean Squared Error for Locally Weighted Regression Boosted with Random Forest is : &#39;</span> <span class="op">+</span> <span class="bu">str</span>(mse_blwr))</span>
<span id="cb56-3"><a href="#cb56-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;The Cross-validated Mean Absolute Error for Locally Weighted Regression Boosted with Random Forest is : &#39;</span> <span class="op">+</span> <span class="bu">str</span>(mae_blwr))</span>
<span id="cb56-4"><a href="#cb56-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;The Cross-validated Coefficient of Determination for Locally Weighted Regression Boosted with Random Forest is : &#39;</span> <span class="op">+</span> <span class="bu">str</span>(r2_blwr))</span></code></pre></div>
<div class="output stream stdout">
<pre><code>The Cross-validated Mean Squared Error for Locally Weighted Regression Boosted with Random Forest is : 142.51517506407464
The Cross-validated Mean Absolute Error for Locally Weighted Regression Boosted with Random Forest is : 9.620449887420262
The Cross-validated Coefficient of Determination for Locally Weighted Regression Boosted with Random Forest is : 0.47975481741057535
</code></pre>
</div>
</div>
<div class="cell code" data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}" id="GZaziG1-AY31" data-outputId="15555f66-6dea-4313-c826-35f4b378ba12">
<div class="sourceCode" id="cb58"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb58-1"><a href="#cb58-1" aria-hidden="true" tabindex="-1"></a>mse_xgbr, mae_xgbr, r2_xgbr <span class="op">=</span> RunModel1(xgbr, x, y, ss, kf)</span>
<span id="cb58-2"><a href="#cb58-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;The Cross-validated Mean Squared Error for Extreme Gradient Regression is : &#39;</span> <span class="op">+</span> <span class="bu">str</span>(mse_xgbr))</span>
<span id="cb58-3"><a href="#cb58-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;The Cross-validated Mean Absolute Error for Extreme Gradient Regression is : &#39;</span> <span class="op">+</span> <span class="bu">str</span>(mae_xgbr))</span>
<span id="cb58-4"><a href="#cb58-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;The Cross-validated Coefficient of Determination for Extreme Gradient Regression is : &#39;</span> <span class="op">+</span> <span class="bu">str</span>(r2_xgbr))</span></code></pre></div>
<div class="output stream stdout">
<pre><code>The Cross-validated Mean Squared Error for Extreme Gradient Regression is : 142.3439960678086
The Cross-validated Mean Absolute Error for Extreme Gradient Regression is : 9.609343509785178
The Cross-validated Coefficient of Determination for Extreme Gradient Regression is : 0.4814444923220341
</code></pre>
</div>
</div>
<div class="cell markdown" id="LDQErmtaPKkP">
<p>Like before, all of the models performed relatively poorly, though much more poorly than with the first data. Again, locally weighted regression performed the best out of the standard nonparametric models. However, even more strangely than with the last data, ordinary linear regression performed marginally better than both the decision tree and random forest regressions, though I suspect with a little optimization, these two could surpass it. The artificial neural network handled this data much better than the last, and achieved a result that competes with the nonparametric models. In this case, both of the boosted versions of the locally weighted regression did perform slightly better than the standard model. The extreme gradient regression again performed the best out of all of the models. Overall, the boosted models perform marginally better than the standard models again. I suspect most of the differences in the results between the two data are result of my variable selection.</p>
</div>
<section id="limitations" class="cell markdown" id="ckJG63f7cn5E">
<h2>Limitations</h2>
<p>As previously mentioned, all of the tested models were ill-suited to both data, and the next priority task is to perform adequate variable selection. Furthermore, it would be beneficial to perform optimization through hyperparameter selection loops, to determine the best construction of each model for each dataset. Overall, I can conclude that boosting is effective, and for this to become more obvious, I suspect that I need to improve it with repeated boostings.</p>
</section>
<section id="repeated-boosting" class="cell markdown" id="2LNHjcCjClTT">
<h3>Repeated Boosting</h3>
<p>To expand on one limitation, I will test a <em>repeated boosting</em> method. Similar to standard boosting, I will boost the boosted locally weighted regression a set amount of times, determined with the 'n_boosts' hyperparameter, using decision trees and random forest. We begin by training our standard loess model on the train data and have it make predictions on the test data, stored in our <em>Fx</em> variable. Next, we again train another standard loess on the train data but have it make predictions on the test data, stored in our <em>Fx_new</em> variable. To get the new y values, we subtract our trained train <em>Fx</em> values from the original y values. Then, we iterate the repeated boostings by having our booster fit the data to the new y values, predict output y values from the train data and them append them to our <em>Fx</em> values, then predict output y values from the test data and append them to our <em>Fx_new</em> values. We then subtract the train output from y and run this as many times as <em>n_boosts</em>.</p>
</section>
<div class="cell code" data-execution_count="27" id="x9uvK0jSdMOw">
<div class="sourceCode" id="cb60"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb60-1"><a href="#cb60-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> blwr(X, y, xnew, kern, tau, intercept, booster, n_boosts):</span>
<span id="cb60-2"><a href="#cb60-2" aria-hidden="true" tabindex="-1"></a>  output <span class="op">=</span> rblwr(X, y, xnew, kern, tau, booster, n_boosts)</span>
<span id="cb60-3"><a href="#cb60-3" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> output </span></code></pre></div>
</div>
<div class="cell code" data-execution_count="28" id="5YU26LwynjTS">
<div class="sourceCode" id="cb61"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb61-1"><a href="#cb61-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> rblwr(X, y, xpred, kern, tau, booster, n_boosts):</span>
<span id="cb61-2"><a href="#cb61-2" aria-hidden="true" tabindex="-1"></a>  Fx <span class="op">=</span> lwr(X, y, X, kern, tau, <span class="va">True</span>)</span>
<span id="cb61-3"><a href="#cb61-3" aria-hidden="true" tabindex="-1"></a>  Fx_new <span class="op">=</span> lwr(X, y, xpred, kern, tau, <span class="va">True</span>)</span>
<span id="cb61-4"><a href="#cb61-4" aria-hidden="true" tabindex="-1"></a>  y_new <span class="op">=</span> y <span class="op">-</span> Fx</span>
<span id="cb61-5"><a href="#cb61-5" aria-hidden="true" tabindex="-1"></a>  output <span class="op">=</span> Fx</span>
<span id="cb61-6"><a href="#cb61-6" aria-hidden="true" tabindex="-1"></a>  output_new <span class="op">=</span> Fx_new</span>
<span id="cb61-7"><a href="#cb61-7" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n_boosts):</span>
<span id="cb61-8"><a href="#cb61-8" aria-hidden="true" tabindex="-1"></a>    booster.fit(X, y_new)</span>
<span id="cb61-9"><a href="#cb61-9" aria-hidden="true" tabindex="-1"></a>    output <span class="op">+=</span> booster.predict(X)</span>
<span id="cb61-10"><a href="#cb61-10" aria-hidden="true" tabindex="-1"></a>    output_new <span class="op">+=</span> booster.predict(xpred)</span>
<span id="cb61-11"><a href="#cb61-11" aria-hidden="true" tabindex="-1"></a>    y_new <span class="op">=</span> y <span class="op">-</span> output</span>
<span id="cb61-12"><a href="#cb61-12" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> output_new</span></code></pre></div>
</div>
<div class="cell code" data-execution_count="40" data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}" id="Dy30aS_f1A-A" data-outputId="4f10b5f3-7439-412c-be39-22dc502a32ec">
<div class="sourceCode" id="cb62"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb62-1"><a href="#cb62-1" aria-hidden="true" tabindex="-1"></a>mse_rblwr, mae_rblwr, r2_rblwr <span class="op">=</span> RunModel2(blwr, x , y, kern<span class="op">=</span>Tricubic, tau<span class="op">=</span><span class="dv">1</span>, scaler<span class="op">=</span>ss, split<span class="op">=</span>kf, booster<span class="op">=</span>dtr, n_boosts<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb62-2"><a href="#cb62-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;The Cross-validated Mean Squared Error for Locally Weighted Regression Repeatedly Boosted 3 times with Decision Trees is : &#39;</span> <span class="op">+</span> <span class="bu">str</span>(mse_rblwr))</span>
<span id="cb62-3"><a href="#cb62-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;The Cross-validated Mean Absolute Error for Locally Weighted Regression Repeatedly Boosted 3 times with Decision Trees is : &#39;</span> <span class="op">+</span> <span class="bu">str</span>(mae_rblwr))</span>
<span id="cb62-4"><a href="#cb62-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;The Cross-validated Coefficient of Determination for Locally Weighted Regression Repeatedly Boosted 3 times with Decision Trees is : &#39;</span> <span class="op">+</span> <span class="bu">str</span>(r2_rblwr))</span></code></pre></div>
<div class="output stream stdout">
<pre><code>The Cross-validated Mean Squared Error for Locally Weighted Regression Repeatedly Boosted 3 times with Decision Trees is : 143.99138019972173
The Cross-validated Mean Absolute Error for Locally Weighted Regression Repeatedly Boosted 3 times with Decision Trees is : 9.646703315207322
The Cross-validated Coefficient of Determination for Locally Weighted Regression Repeatedly Boosted 3 times with Decision Trees is : 0.4744820648346198
</code></pre>
</div>
</div>
<div class="cell code" data-execution_count="43" data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}" id="-APVaK8wzV_1" data-outputId="357ca22b-bd8a-4dbf-8450-94591a0789e1">
<div class="sourceCode" id="cb64"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb64-1"><a href="#cb64-1" aria-hidden="true" tabindex="-1"></a>mse_rblwr, mae_rblwr, r2_rblwr <span class="op">=</span> RunModel2(blwr, x , y, kern<span class="op">=</span>Tricubic, tau<span class="op">=</span><span class="dv">1</span>, scaler<span class="op">=</span>ss, split<span class="op">=</span>kf, booster<span class="op">=</span>rfr, n_boosts<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb64-2"><a href="#cb64-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;The Cross-validated Mean Squared Error for Locally Weighted Regression Repeatedly Boosted 2 times with Random Forest is : &#39;</span> <span class="op">+</span> <span class="bu">str</span>(mse_rblwr))</span>
<span id="cb64-3"><a href="#cb64-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;The Cross-validated Mean Absolute Error for Locally Weighted Regression Repeatedly Boosted 2 times with Random Forest is : &#39;</span> <span class="op">+</span> <span class="bu">str</span>(mae_rblwr))</span>
<span id="cb64-4"><a href="#cb64-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;The Cross-validated Coefficient of Determination for Locally Weighted Regression Repeatedly Boosted 2 times with Random Forest is : &#39;</span> <span class="op">+</span> <span class="bu">str</span>(r2_rblwr))</span></code></pre></div>
<div class="output stream stdout">
<pre><code>The Cross-validated Mean Squared Error for Locally Weighted Regression Repeatedly Boosted 2 times with Random Forest is : 141.78840206960217
The Cross-validated Mean Absolute Error for Locally Weighted Regression Repeatedly Boosted 2 times with Random Forest is : 9.597334091698368
The Cross-validated Coefficient of Determination for Locally Weighted Regression Repeatedly Boosted 2 times with Random Forest is : 0.48274131795687075
</code></pre>
</div>
</div>
<div class="cell code" data-execution_count="44" data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}" id="rQynvssw4G_D" data-outputId="717ba35e-71cd-4f8f-c4df-9c232bd0cba3">
<div class="sourceCode" id="cb66"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb66-1"><a href="#cb66-1" aria-hidden="true" tabindex="-1"></a>mse_rblwr, mae_rblwr, r2_rblwr <span class="op">=</span> RunModel2(blwr, x , y, kern<span class="op">=</span>Tricubic, tau<span class="op">=</span><span class="dv">1</span>, scaler<span class="op">=</span>ss, split<span class="op">=</span>kf, booster<span class="op">=</span>xgbr, n_boosts<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb66-2"><a href="#cb66-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;The Cross-validated Mean Squared Error for Locally Weighted Regression Repeatedly Boosted 2 times with Extreme Gradient Boosting is : &#39;</span> <span class="op">+</span> <span class="bu">str</span>(mse_rblwr))</span>
<span id="cb66-3"><a href="#cb66-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;The Cross-validated Mean Absolute Error for Locally Weighted Regression Repeatedly Boosted 2 times with Extreme Gradient Boosting is : &#39;</span> <span class="op">+</span> <span class="bu">str</span>(mae_rblwr))</span>
<span id="cb66-4"><a href="#cb66-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;The Cross-validated Coefficient of Determination for Locally Weighted Regression Repeatedly Boosted 2 times with Extreme Gradient Boosting is : &#39;</span> <span class="op">+</span> <span class="bu">str</span>(r2_rblwr))</span></code></pre></div>
<div class="output stream stdout">
<pre><code>The Cross-validated Mean Squared Error for Locally Weighted Regression Repeatedly Boosted 2 times with Extreme Gradient Boosting is : 142.55907278216768
The Cross-validated Mean Absolute Error for Locally Weighted Regression Repeatedly Boosted 2 times with Extreme Gradient Boosting is : 9.515555039478073
The Cross-validated Coefficient of Determination for Locally Weighted Regression Repeatedly Boosted 2 times with Extreme Gradient Boosting is : 0.4793726164390958
</code></pre>
</div>
</div>
<div class="cell markdown" id="S9NrgjyV1hE9">
<p>As expected, the XGBoost performed better than the Random Forest and Decision Tree boosters; however, the repeated boostings did not perform much better than the standard boostings. I suspect this is more confirmation that the problem is our variable selection with this particular data.</p>
</div>
<section id="lightgbm" class="cell markdown" id="AI1v-FUE1hJw">
<h3>LightGBM</h3>
<p>LightGBM is a proprietary GBDT (gradient boosting decision tree) framework designed by Microsoft. The algorithm is supported by decision tree regressors and classifiers, depending on the task. However, while other boosting algorithms split the trees via depth or level-wise, LightGBM splits the tree via <em>simplest fit</em>, or leaf-wise. In other words, the algorithm chooses the leaf with the largest loss (distance between the current output and the expected output) to grow off of. The he most time-intensive part in training gradient boosted decision trees in finding the optimal split points. LightGBM is renowned as a speedy alternative to the traditional methods of finding the split points on the sorted feature values through its use of <em>histogram-based buckets</em>. This method sorts the observation values into discrete bins which are used to create feature histograms during training, using the number of bins times the number of features to find split points. LightGBM is intended for a large data, as it tends to overfit small data having less than 10,000 rows. Its important parameters are:</p>
<ul>
<li>max_depth: determines the limit on the depth of a tree (controls overfitting)</li>
<li>bagging_fraction: determines the proportion of the data to be modeled in each iteration</li>
<li>num_iterations: determines the number of iterations</li>
<li>num_leaves: determines the number of leaves per tree (ideally less than the square of max_depth)</li>
</ul>
<p>I will perform a LightGBM regression on the concrete data to test its efficacy, using the same variables as in the previous regressions. My method takes inspiration from the "House Price Regression with LightGBM" article on Kaggle.</p>
</section>
<div class="cell markdown" id="fXPg-GVjAFJy">
<p>Here, we want to create a dictionary with all of the relevant parameters. We want to train the model and boost it with decision trees to perform a regression analysis. I will choose a max depth of 4 and the number of leaves to be 12, such that it is less than the square of the max depth, and iterate 10,000 times. I will also bag 70% of the data</p>
</div>
<div class="cell code" data-execution_count="51" id="9RXI_xRJ0T2P">
<div class="sourceCode" id="cb68"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb68-1"><a href="#cb68-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> lightgbm</span>
<span id="cb68-2"><a href="#cb68-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-3"><a href="#cb68-3" aria-hidden="true" tabindex="-1"></a>hyper_params <span class="op">=</span> {</span>
<span id="cb68-4"><a href="#cb68-4" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;task&#39;</span>: <span class="st">&#39;train&#39;</span>,</span>
<span id="cb68-5"><a href="#cb68-5" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;boosting_type&#39;</span>: <span class="st">&#39;gbdt&#39;</span>,</span>
<span id="cb68-6"><a href="#cb68-6" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;objective&#39;</span>: <span class="st">&#39;regression&#39;</span>,</span>
<span id="cb68-7"><a href="#cb68-7" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;metric&#39;</span>: [<span class="st">&#39;l1&#39;</span>,<span class="st">&#39;l2&#39;</span>],</span>
<span id="cb68-8"><a href="#cb68-8" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;learning_rate&#39;</span>: <span class="fl">0.005</span>,</span>
<span id="cb68-9"><a href="#cb68-9" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;feature_fraction&#39;</span>: <span class="fl">0.9</span>,</span>
<span id="cb68-10"><a href="#cb68-10" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;bagging_fraction&#39;</span>: <span class="fl">0.7</span>,</span>
<span id="cb68-11"><a href="#cb68-11" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;bagging_freq&#39;</span>: <span class="dv">10</span>,</span>
<span id="cb68-12"><a href="#cb68-12" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;verbose&#39;</span>: <span class="dv">0</span>,</span>
<span id="cb68-13"><a href="#cb68-13" aria-hidden="true" tabindex="-1"></a>    <span class="st">&quot;max_depth&quot;</span>: <span class="dv">4</span>,</span>
<span id="cb68-14"><a href="#cb68-14" aria-hidden="true" tabindex="-1"></a>    <span class="st">&quot;num_leaves&quot;</span>: <span class="dv">12</span>,  </span>
<span id="cb68-15"><a href="#cb68-15" aria-hidden="true" tabindex="-1"></a>    <span class="st">&quot;num_iterations&quot;</span>: <span class="dv">10000</span></span>
<span id="cb68-16"><a href="#cb68-16" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
</div>
<div class="cell code" data-execution_count="52" id="QZjXzHpFBRbB">
<div class="sourceCode" id="cb69"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb69-1"><a href="#cb69-1" aria-hidden="true" tabindex="-1"></a>lgbmr <span class="op">=</span> lightgbm.LGBMRegressor(<span class="op">**</span>hyper_params)</span></code></pre></div>
</div>
<div class="cell markdown" id="MvF4NgQ2ArZT">
<p>Here, we want to define our LightGBM regressor and run our cross-validated analysis</p>
</div>
<div class="cell code" data-execution_count="53" data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}" id="147303rlBE86" data-outputId="e7299d1d-493f-40f8-b5f2-1202186fd0d4">
<div class="sourceCode" id="cb70"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb70-1"><a href="#cb70-1" aria-hidden="true" tabindex="-1"></a>mse_lgbmr, mae_lgbmr, r2_lgbmr <span class="op">=</span> RunModel1(lgbmr, x, y, ss, kf)</span>
<span id="cb70-2"><a href="#cb70-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;The Cross-validated Mean Squared Error for LightGBM Regression is : &#39;</span> <span class="op">+</span> <span class="bu">str</span>(mse_lgbmr))</span>
<span id="cb70-3"><a href="#cb70-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;The Cross-validated Mean Absolute Error for LightGBM Regression is : &#39;</span> <span class="op">+</span> <span class="bu">str</span>(mae_lgbmr))</span>
<span id="cb70-4"><a href="#cb70-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;The Cross-validated Coefficient of Determination for LightGBM Regression is : &#39;</span> <span class="op">+</span> <span class="bu">str</span>(r2_lgbmr))</span></code></pre></div>
<div class="output stream stderr">
<pre><code>/usr/local/lib/python3.7/dist-packages/lightgbm/engine.py:118: UserWarning: Found `num_iterations` in params. Will use it instead of argument
  warnings.warn(&quot;Found `{}` in params. Will use it instead of argument&quot;.format(alias))
/usr/local/lib/python3.7/dist-packages/lightgbm/engine.py:118: UserWarning: Found `num_iterations` in params. Will use it instead of argument
  warnings.warn(&quot;Found `{}` in params. Will use it instead of argument&quot;.format(alias))
/usr/local/lib/python3.7/dist-packages/lightgbm/engine.py:118: UserWarning: Found `num_iterations` in params. Will use it instead of argument
  warnings.warn(&quot;Found `{}` in params. Will use it instead of argument&quot;.format(alias))
/usr/local/lib/python3.7/dist-packages/lightgbm/engine.py:118: UserWarning: Found `num_iterations` in params. Will use it instead of argument
  warnings.warn(&quot;Found `{}` in params. Will use it instead of argument&quot;.format(alias))
/usr/local/lib/python3.7/dist-packages/lightgbm/engine.py:118: UserWarning: Found `num_iterations` in params. Will use it instead of argument
  warnings.warn(&quot;Found `{}` in params. Will use it instead of argument&quot;.format(alias))
/usr/local/lib/python3.7/dist-packages/lightgbm/engine.py:118: UserWarning: Found `num_iterations` in params. Will use it instead of argument
  warnings.warn(&quot;Found `{}` in params. Will use it instead of argument&quot;.format(alias))
/usr/local/lib/python3.7/dist-packages/lightgbm/engine.py:118: UserWarning: Found `num_iterations` in params. Will use it instead of argument
  warnings.warn(&quot;Found `{}` in params. Will use it instead of argument&quot;.format(alias))
/usr/local/lib/python3.7/dist-packages/lightgbm/engine.py:118: UserWarning: Found `num_iterations` in params. Will use it instead of argument
  warnings.warn(&quot;Found `{}` in params. Will use it instead of argument&quot;.format(alias))
/usr/local/lib/python3.7/dist-packages/lightgbm/engine.py:118: UserWarning: Found `num_iterations` in params. Will use it instead of argument
  warnings.warn(&quot;Found `{}` in params. Will use it instead of argument&quot;.format(alias))
/usr/local/lib/python3.7/dist-packages/lightgbm/engine.py:118: UserWarning: Found `num_iterations` in params. Will use it instead of argument
  warnings.warn(&quot;Found `{}` in params. Will use it instead of argument&quot;.format(alias))
</code></pre>
</div>
<div class="output stream stdout">
<pre><code>The Cross-validated Mean Squared Error for LightGBM Regression is : 156.01308387462663
The Cross-validated Mean Absolute Error for LightGBM Regression is : 9.901618206845555
The Cross-validated Coefficient of Determination for LightGBM Regression is : 0.42980606055413545
</code></pre>
</div>
</div>
<div class="cell markdown" id="4MuG-nBaCotj">
<p>This booster seemed to perform the worst out of all of the boosting methods; likely again due to our variable selection, but I am merely surmising this. Though I do not show it in this notebook, increasing the max_depth and the num_leaves seemed to make the model perform worse.</p>
</div>
<section id="citations" class="cell markdown" id="UFbnrHAu7Hax">
<h3>Citations</h3>
<ul>
<li><p>Lasmith. (2021, August 23). House price regression with lightgbm. Kaggle. Retrieved March 6, 2022, from <a href="https://www.kaggle.com/lasmith/house-price-regression-with-lightgbm" class="uri">https://www.kaggle.com/lasmith/house-price-regression-with-lightgbm</a></p></li>
<li><p>Shreyanshi. (2021, December 22). Lightgbm (Light Gradient Boosting Machine). GeeksforGeeks. Retrieved March 6, 2022, from <a href="https://www.geeksforgeeks.org/lightgbm-light-gradient-boosting-machine/" class="uri">https://www.geeksforgeeks.org/lightgbm-light-gradient-boosting-machine/</a></p></li>
<li><p>Team, D. S., &amp; Team, D. S. (2020, November 15). What is light GBM? - machine learning. DATA SCIENCE. Retrieved March 6, 2022, from <a href="https://datascience.eu/machine-learning/1-what-is-light-gbm/" class="uri">https://datascience.eu/machine-learning/1-what-is-light-gbm/</a></p></li>
<li><p>What is XGBoost? NVIDIA Data Science Glossary. (n.d.). Retrieved March 6, 2022, from <a href="https://www.nvidia.com/en-us/glossary/data-science/xgboost/" class="uri">https://www.nvidia.com/en-us/glossary/data-science/xgboost/</a></p></li>
<li><p>z_ai. (2021, September 26). What is boosting in machine learning? Medium. Retrieved March 6, 2022, from <a href="https://towardsdatascience.com/what-is-boosting-in-machine-learning-2244aa196682" class="uri">https://towardsdatascience.com/what-is-boosting-in-machine-learning-2244aa196682</a></p></li>
</ul>
</section>
</body>
</html>
